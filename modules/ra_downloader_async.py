# modules/ra_downloader_async.py
import os
import zipfile
import asyncio
import logging
from pathlib import Path
from typing import Set, Dict
from datetime import datetime
import aiohttp
import json

# ARCHIVE_URL –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –∑–∞–¥–∞–≤–∞—Ç—å –≤ modules/ra_config.py –∏–ª–∏ —á–µ—Ä–µ–∑ env RA_ARCHIVE_URL
ARCHIVE_URL = os.getenv("RA_ARCHIVE_URL", "")  # —É–∫–∞–∂–∏ —Å–≤–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π URL –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏
DATA_DIR = Path(os.getenv("RA_DATA_DIR", "data"))
LOCAL_ZIP = DATA_DIR / "RaSvet.zip"
EXTRACT_DIR = DATA_DIR / "RaSvet"
EXTRACT_META = DATA_DIR / "RaSvet.extract.meta"
META_JSON = DATA_DIR / "RaSvet.meta.json"

DATA_DIR.mkdir(parents=True, exist_ok=True)
EXTRACT_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger("RaSvetDownloaderAsync")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
handler.setFormatter(formatter)
if not logger.handlers:
    logger.addHandler(handler)

class KnowledgeBase:
    def __init__(self):
        self.documents: Dict[str, Dict] = {}

    async def load_from_folder(self, folder: Path):
        self.documents = {}
        try:
            for file in Path(folder).rglob("*"):
                if file.is_file() and file.suffix in [".txt", ".md", ".json"]:
                    try:
                        self.documents[file.name] = {
                            "content": file.read_text(encoding="utf-8"),
                            "mtime": datetime.fromtimestamp(file.stat().st_mtime)
                        }
                    except Exception as e:
                        logger.warning(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {file}: {e}")
            logger.info(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–Ω–∞–Ω–∏–π: {len(self.documents)} —Ñ–∞–π–ª–æ–≤")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑ –ø–∞–ø–∫–∏ {folder}: {e}")

    async def ask(self, question: str, user_id=None) -> str:
        if not question:
            return None
        answers = []
        sorted_docs = sorted(self.documents.items(), key=lambda x: x[1].get("mtime", datetime.min), reverse=True)
        for fname, meta in sorted_docs:
            try:
                if question.lower() in meta["content"].lower():
                    snippet = meta["content"][:500].replace("\n", " ")
                    answers.append(f"[{fname}] {snippet}...")
            except Exception:
                continue
        return "\n\n".join(answers[:5]) if answers else None

class RaSvetDownloaderAsync:
    def __init__(self):
        self.knowledge = KnowledgeBase()
        self.extracted_files: Set[str] = set()
        self.meta_data: Dict[str, Dict] = {}
        if EXTRACT_META.exists():
            try:
                self.extracted_files = set(line.strip() for line in EXTRACT_META.read_text().splitlines() if line.strip())
            except Exception:
                self.extracted_files = set()
        if META_JSON.exists():
            try:
                self.meta_data = json.loads(META_JSON.read_text())
            except Exception:
                self.meta_data = {}

    async def download_async(self):
        await self._download_archive_if_needed()
        await self._safe_extract_incremental()
        await self.knowledge.load_from_folder(EXTRACT_DIR)

    async def _download_archive_if_needed(self):
        if not ARCHIVE_URL:
            logger.warning("ARCHIVE_URL not configured ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ")
            return

        try:
            timeout = aiohttp.ClientTimeout(total=60)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                # HEAD may not return Content-Length ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º gracefully
                try:
                    async with session.head(ARCHIVE_URL) as resp:
                        resp.raise_for_status()
                        remote_size = int(resp.headers.get("Content-Length") or 0)
                except Exception:
                    remote_size = 0

                local_size = LOCAL_ZIP.stat().st_size if LOCAL_ZIP.exists() else 0
                need_download = (local_size == 0) or (remote_size and local_size != remote_size)

                if not need_download:
                    logger.info("‚ÑπÔ∏è –ê—Ä—Ö–∏–≤ –∞–∫—Ç—É–∞–ª–µ–Ω, —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ")
                    return

                logger.info(f"‚¨áÔ∏è –ù–∞—á–∏–Ω–∞—é —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞ RaSvet ({remote_size / 1024:.1f} KB)" if remote_size else "‚¨áÔ∏è –ù–∞—á–∏–Ω–∞—é —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞ RaSvet")
                async with session.get(ARCHIVE_URL) as resp:
                    resp.raise_for_status()
                    with open(LOCAL_ZIP, "wb") as f:
                        async for chunk in resp.content.iter_chunked(32*1024):
                            f.write(chunk)
                            await asyncio.sleep(0)
                logger.info("‚úÖ –ê—Ä—Ö–∏–≤ —Å–∫–∞—á–∞–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–µ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –∞—Ä—Ö–∏–≤–∞: {e}")

    async def _safe_extract_incremental(self):
        EXTRACT_DIR.mkdir(parents=True, exist_ok=True)
        new_files = set()
        if not LOCAL_ZIP.exists():
            logger.warning("‚ö† –ù–µ—Ç –∞—Ä—Ö–∏–≤–∞ –¥–ª—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏")
            return

        try:
            with zipfile.ZipFile(LOCAL_ZIP, 'r') as zip_ref:
                for member in zip_ref.infolist():
                    try:
                        meta = self.meta_data.get(member.filename, {})
                        size_changed = meta.get("size") != member.file_size
                        if member.filename in self.extracted_files and not size_changed:
                            continue
                        zip_ref.extract(member, EXTRACT_DIR)
                        logger.info(f"üìÇ –ù–æ–≤—ã–π –∏–ª–∏ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ñ–∞–π–ª —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω: {member.filename}")
                        new_files.add(member.filename)
                        self.meta_data[member.filename] = {"size": member.file_size, "mtime": datetime.now().isoformat()}
                        await asyncio.sleep(0)
                    except zipfile.BadZipFile:
                        logger.error("‚ùå –ê—Ä—Ö–∏–≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ member")
                        continue
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–µ {member.filename}: {e}")
        except zipfile.BadZipFile:
            logger.error("‚ùå –ê—Ä—Ö–∏–≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞")
            return
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–µ –∞—Ä—Ö–∏–≤–∞: {e}")
            return

        if new_files:
            self.extracted_files.update(new_files)
            try:
                EXTRACT_META.write_text("\n".join(sorted(self.extracted_files)), encoding="utf-8")
                META_JSON.write_text(json.dumps(self.meta_data, ensure_ascii=False, indent=2), encoding="utf-8")
                logger.info(f"üåû –û–±–Ω–æ–≤–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(new_files)}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏: {e}")

        try:
            if LOCAL_ZIP.exists():
                LOCAL_ZIP.unlink()
                logger.info("üßπ –ê—Ä—Ö–∏–≤ —É–¥–∞–ª—ë–Ω –ø–æ—Å–ª–µ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π –∞—Ä—Ö–∏–≤: {e}")
