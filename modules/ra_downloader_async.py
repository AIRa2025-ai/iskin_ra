# modules/ra_downloader_async.py
import os
import zipfile
import asyncio
import logging
from pathlib import Path
from typing import Set, Dict
from datetime import datetime
import aiohttp
import json

ARCHIVE_URL = "https://mega.nz/file/FlQ0ET4J#9gJjCBnj5uYn5bJYYMfPiN3BTvWz8el8leCWQPZvrUg"
DATA_DIR = Path("/app/data_disk")
LOCAL_ZIP = DATA_DIR / "RaSvet.zip"
EXTRACT_DIR = DATA_DIR / "RaSvet"
EXTRACT_META = DATA_DIR / "RaSvet.extract.meta"
META_JSON = DATA_DIR / "RaSvet.meta.json"  # —Ö—Ä–∞–Ω–∏—Ç –¥–∞—Ç—É –∏ size —Ñ–∞–π–ª–æ–≤ –¥–ª—è incremental

DATA_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger("RaSvetDownloaderAsync")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)

class KnowledgeBase:
    def __init__(self):
        self.documents: Dict[str, Dict] = {}  # filename -> {"content": str, "mtime": datetime}

    async def load_from_folder(self, folder: Path):
        self.documents = {}
        for file in folder.rglob("*"):
            if file.is_file() and file.suffix in [".txt", ".md", ".json"]:
                try:
                    self.documents[file.name] = {
                        "content": file.read_text(encoding="utf-8"),
                        "mtime": datetime.fromtimestamp(file.stat().st_mtime)
                    }
                except Exception as e:
                    logger.warning(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {file}: {e}")
        logger.info(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–Ω–∞–Ω–∏–π: {len(self.documents)} —Ñ–∞–π–ª–æ–≤")

    async def ask(self, question: str, user_id=None) -> str:
        answers = []
        sorted_docs = sorted(self.documents.items(), key=lambda x: x[1]["mtime"], reverse=True)
        for fname, meta in sorted_docs:
            if question.lower() in meta["content"].lower():
                snippet = meta["content"][:500].replace("\n", " ")
                answers.append(f"[{fname}] {snippet}...")
        return "\n\n".join(answers[:5]) if answers else None

class RaSvetDownloaderAsync:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ incremental update –∞—Ä—Ö–∏–≤–∞"""
    def __init__(self):
        self.knowledge = KnowledgeBase()
        self.extracted_files: Set[str] = set()
        self.meta_data: Dict[str, Dict] = {}
        if EXTRACT_META.exists():
            self.extracted_files = set(line.strip() for line in EXTRACT_META.read_text().splitlines() if line.strip())
        if META_JSON.exists():
            try:
                self.meta_data = json.loads(META_JSON.read_text())
            except Exception:
                self.meta_data = {}

    async def download_async(self):
        await self._download_archive_if_needed()
        await self._safe_extract_incremental()
        await self.knowledge.load_from_folder(EXTRACT_DIR)

    async def _download_archive_if_needed(self):
        """–°–∫–∞—á–∏–≤–∞–µ–º –∞—Ä—Ö–∏–≤ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–ª–∏ –∏–∑–º–µ–Ω–∏–ª—Å—è —Ä–∞–∑–º–µ—Ä"""
        async with aiohttp.ClientSession() as session:
            async with session.head(ARCHIVE_URL) as resp:
                resp.raise_for_status()
                remote_size = int(resp.headers.get("Content-Length", 0))
        local_size = LOCAL_ZIP.stat().st_size if LOCAL_ZIP.exists() else 0
        if local_size != remote_size:
            logger.info(f"‚¨áÔ∏è –ù–∞—á–∏–Ω–∞—é —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞ RaSvet ({remote_size / 1024:.1f} KB)")
            async with aiohttp.ClientSession() as session:
                async with session.get(ARCHIVE_URL) as resp:
                    resp.raise_for_status()
                    with open(LOCAL_ZIP, "wb") as f:
                        async for chunk in resp.content.iter_chunked(32*1024):
                            f.write(chunk)
                            await asyncio.sleep(0)
            logger.info("‚úÖ –ê—Ä—Ö–∏–≤ —Å–∫–∞—á–∞–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–µ")
        else:
            logger.info("‚ÑπÔ∏è –ê—Ä—Ö–∏–≤ –∞–∫—Ç—É–∞–ª–µ–Ω, —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ")

    async def _safe_extract_incremental(self):
        EXTRACT_DIR.mkdir(parents=True, exist_ok=True)
        new_files = set()
        if not LOCAL_ZIP.exists():
            logger.warning("‚ö† –ù–µ—Ç –∞—Ä—Ö–∏–≤–∞ –¥–ª—è —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏")
            return

        try:
            with zipfile.ZipFile(LOCAL_ZIP, 'r') as zip_ref:
                for member in zip_ref.infolist():
                    meta = self.meta_data.get(member.filename, {})
                    size_changed = meta.get("size") != member.file_size
                    if member.filename in self.extracted_files and not size_changed:
                        continue
                    zip_ref.extract(member, EXTRACT_DIR)
                    logger.info(f"üìÇ –ù–æ–≤—ã–π –∏–ª–∏ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ñ–∞–π–ª —Ä–∞—Å–ø–∞–∫–æ–≤–∞–Ω: {member.filename}")
                    new_files.add(member.filename)
                    self.meta_data[member.filename] = {"size": member.file_size, "mtime": datetime.now().isoformat()}
                    await asyncio.sleep(0)
        except zipfile.BadZipFile:
            logger.error("‚ùå –ê—Ä—Ö–∏–≤ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω, —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞")
            return

        if new_files:
            self.extracted_files.update(new_files)
            EXTRACT_META.write_text("\n".join(self.extracted_files))
            META_JSON.write_text(json.dumps(self.meta_data, indent=2))
            logger.info(f"üåû –û–±–Ω–æ–≤–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(new_files)}")

        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –∞—Ä—Ö–∏–≤ –ø–æ—Å–ª–µ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏
        if LOCAL_ZIP.exists():
            LOCAL_ZIP.unlink()
            logger.info("üßπ –ê—Ä—Ö–∏–≤ —É–¥–∞–ª—ë–Ω –ø–æ—Å–ª–µ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏")
