# modules/wanderer_v2.py
# -*- coding: utf-8 -*-
"""
RaSvet Wanderer 2.0 ‚Äî –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Å–±–æ—Ä—â–∏–∫ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π:
- –æ–±—Ö–æ–¥ —Å—Å—ã–ª–æ–∫ –≤–Ω—É—Ç—Ä–∏ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
- —Å–±–æ—Ä –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (title, description, keywords)
- —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –∫—Ä–∞—Ç–∫–∏—Ö –≤—ã–¥–µ—Ä–∂–µ–∫
- –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π
"""

import os, time, random, logging, json
from pathlib import Path
from urllib.parse import urljoin, urlparse  # noqa: F401

import requests
from bs4 import BeautifulSoup
import tldextract

USER_AGENT = "RaSvetBot/2.0 (+https://example.invalid)"
TIMEOUT = 15
MAX_DEPTH = 2  # –º–∞–∫—Å –≥–ª—É–±–∏–Ω–∞ –æ–±—Ö–æ–¥–∞ —Å—Å—ã–ª–æ–∫

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–æ–≤ ---
LOG_DIR = Path("RaSvet/–±—Ä–æ–¥—è–≥–∞/logs")
LOG_DIR.mkdir(parents=True, exist_ok=True)
logging.basicConfig(
    filename=LOG_DIR / "wanderer.log",
    level=logging.INFO,
    format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s",
)

# --- –ü–æ–ª–µ–∑–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
def _allowed_domains():
    """–†–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–µ –¥–æ–º–µ–Ω—ã —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    raw = os.getenv("ALLOWED_DOMAINS", "")
    return {d.strip().lower() for d in raw.split(",") if d.strip()}

def _domain_allowed(url: str) -> bool:
    ext = tldextract.extract(url)
    dom = ".".join(part for part in [ext.domain, ext.suffix] if part)
    return dom.lower() in _allowed_domains()

def _respectful_get(url: str):
    headers = {"User-Agent": USER_AGENT}
    return requests.get(url, headers=headers, timeout=TIMEOUT)

def _safe_text(s: str, limit=5000):
    s = " ".join(s.split())
    return s[:limit]

def _extract_meta(soup):
    """–°–æ–±–∏—Ä–∞–µ–º title, description, keywords"""
    meta = {}
    meta["title"] = soup.title.string.strip() if soup.title else ""
    desc = soup.find("meta", attrs={"name":"description"})
    meta["description"] = desc["content"].strip() if desc and "content" in desc.attrs else ""
    keys = soup.find("meta", attrs={"name":"keywords"})
    meta["keywords"] = keys["content"].strip() if keys and "content" in keys.attrs else ""
    return meta

def _collect_links(soup, base_url):
    """–°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –≤–Ω—É—Ç—Ä–∏ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤"""
    links = set()
    for a in soup.find_all("a", href=True):
        href = urljoin(base_url, a["href"])
        if _domain_allowed(href):
            links.add(href)
    return list(links)

def crawl_page(url: str, out_dir="RaSvet/–±—Ä–æ–¥—è–≥–∞/–Ω–∞–±–ª—é–¥–µ–Ω–∏—è") -> dict:
    """–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    Path(out_dir).mkdir(parents=True, exist_ok=True)
    try:
        r = _respectful_get(url)
        if r.status_code != 200 or "text/html" not in r.headers.get("Content-Type",""):
            return {"status":"bad_response", "code": r.status_code, "url":url}

        soup = BeautifulSoup(r.text, "html.parser")
        for tag in soup(["script","style","noscript"]):
            tag.extract()

        text_full = _safe_text(soup.get_text("\n"))
        meta = _extract_meta(soup)
        links = _collect_links(soup, url)

        ts = time.strftime("%Y-%m-%d_%H-%M-%S")
        record = {
            "timestamp": ts,
            "url": url,
            "meta": meta,
            "excerpt": text_full[:1000],
            "full_text": text_full,
            "links": links
        }
        out_file = Path(out_dir) / f"{ts}.json"
        with open(out_file, "w", encoding="utf-8") as f:
            json.dump(record, f, ensure_ascii=False, indent=2)

        logging.info(f"üåê –ë—Ä–æ–¥—è–≥–∞2: –ø—Ä–æ—á–∏—Ç–∞–ª {url} ‚Üí {out_file}")
        return {"status":"ok", "file": str(out_file), "meta": meta, "links": links}

    except requests.RequestException as e:
        logging.warning(f"‚ö†Ô∏è –ë—Ä–æ–¥—è–≥–∞ —Å–µ—Ç—å: {e}")
        return {"status":"network_error", "url":url, "error": str(e)}

def wander(seed_urls: list[str], out_dir="RaSvet/–±—Ä–æ–¥—è–≥–∞/–Ω–∞–±–ª—é–¥–µ–Ω–∏—è", depth=0, max_depth=MAX_DEPTH, visited=None):
    """–û–±—Ö–æ–¥ seed-—Å—Ç—Ä–∞–Ω–∏—Ü —Å –ø–µ—Ä–µ—Ö–æ–¥–æ–º –ø–æ —Å—Å—ã–ª–∫–∞–º"""
    if visited is None:
        visited = set()
    if not seed_urls or depth > max_depth:
        return

    url = random.choice(seed_urls)
    if url in visited:
        return
    visited.add(url)

    result = crawl_page(url, out_dir)
    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏–¥—ë–º –ø–æ —Å—Å—ã–ª–∫–∞–º –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    if result.get("links"):
        for link in result["links"]:
            wander([link], out_dir=out_dir, depth=depth+1, max_depth=max_depth, visited=visited)
    return visited

# --- –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ---
if __name__ == "__main__":
    seeds = ["https://example.com"]  # –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–∏ —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ URL
    visited_pages = wander(seeds)
    print(f"üåü –ü–æ—Å–µ—â–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(visited_pages)}")
