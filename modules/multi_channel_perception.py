# modules/multi_channel_perception.py
# –ù–∞–≤—ã–∫ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ì–ª–∞–∑–∞ –†–∞
# –°–æ–∑–¥–∞–Ω –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ ¬´–†–∞–°–≤–µ—Ç¬ª
import time
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from textblob import TextBlob

class MultiChannelPerception:
    def __init__(self, logs, sensitivity=0.7, event_bus=None, thinker=None, min_interval=60):
        self.logs = logs
        self.sensitivity = sensitivity  # —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ ¬´–º—É—Å–æ—Ä–Ω—ã–º¬ª –≤–∏–±—Ä–∞—Ü–∏—è–º
        self.event_bus = event_bus
        self.thinker = thinker
        self.heart_reactor = heart_reactor
        self.min_interval = min_interval  # ‚è≥ –º–∏–Ω–∏–º—É–º —Å–µ–∫—É–Ω–¥ –º–µ–∂–¥—É —Å–∫–∞–Ω–∞–º–∏
        self._last_scan_ts = 0
    async def fetch(self, session, url):
        try:
            async with session.get(url, timeout=15) as resp:
                text = await resp.text()
                return text
        except Exception as e:
            return f"ERROR: {e}"

    async def scan_sources(self, urls):
        results = []
        now = time.time()

        if now - self._last_scan_ts < self.min_interval:
            return {
                "status": "skipped",
                "reason": "scan_rate_limited",
                "next_scan_in": int(self.min_interval - (now - self._last_scan_ts))
            }

        self._last_scan_ts = now
        
        if hasattr(self.logs, "heart_reactor"):
            for r in results:
                if r.get("insights"):
                    self.logs.heart_reactor.send_event(
                        f"üëÅ –í–æ—Å–ø—Ä–∏—è—Ç–∏–µ: –Ω–∞–π–¥–µ–Ω —Å–∏–≥–Ω–∞–ª –∏–∑ {r['source']}"
                    )
                    
    async with aiohttp.ClientSession() as session:
        tasks = [self.fetch(session, u) for u in urls]
        pages = await asyncio.gather(*tasks)

        for url, page in zip(urls, pages):
            clean = self.clean_noise(page)
            insights = self.extract_insights(clean)
            results.append({
                "source": url,
                "raw_len": len(page),
                "clean_len": len(clean),
                "insights": insights
            })
        # üîî –°–æ–æ–±—â–∞–µ–º –º–∏—Ä—É –æ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–∏
        if self.event_bus:
            await self.event_bus.emit(
                "perception_update",
                {
                    "channels": len(urls),
                    "signals": results
                }
            )

        # üß† –ü–µ—Ä–µ–¥–∞—ë–º –º—ã—Å–ª–∏ –º—ã—Å–ª–∏—Ç–µ–ª—é
        if self.thinker:
            await self.thinker.process_world_message(
                {
                    "type": "perception",
                    "data": results
                }
            )
        return results

    def clean_noise(self, text):
        """
        –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –º—É—Å–æ—Ä–∞:
        - —Å–ª–∏—à–∫–æ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –ø—Ä–∏–≥–ª—É—à–∞—é—Ç—Å—è
        - —Å–ø–∞–º —É–±–∏—Ä–∞–µ—Ç—Å—è
        - –ª–∏—à–Ω—è—è —Ä–µ–∫–ª–∞–º–∞ —Å—Ç–∏—Ä–∞–µ—Ç—Å—è
        """
        sentiment = TextBlob(text).sentiment.polarity
        if sentiment < -self.sensitivity:
            return ""
        return text

    def extract_insights(self, text):
        """
        –í—ã–¥–µ–ª–µ–Ω–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–≥–æ:
        - —Ä–µ–¥–∫–∏–µ —Ñ—Ä–∞–∑—ã
        - —Å–º—ã—Å–ª–æ–≤—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏
        - —Å–∏–ª—å–Ω—ã–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã
        - –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Ü–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        if len(text) < 100:
            return None

        soup = BeautifulSoup(text, "html.parser")
        words = soup.get_text().split()

        rare = [w for w in words if len(w) > 8][:10]

        return {
            "rare_words": rare,
            "sample": " ".join(words[:50])
        }
        
        if insights and self.heart_reactor:
            self.heart_reactor.send_event(
                "üåê –í–æ—Å–ø—Ä–∏—è—Ç–∏–µ –º–∏—Ä–∞: –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∑–Ω–∞—á–∏–º—ã–µ —Å–∏–≥–Ω–∞–ª—ã"
            )
