# core/gpt_module.py
import os
import aiohttp
import logging
import asyncio
import json
from datetime import datetime, timedelta
from github_commit import create_commit_push  # –æ—Å—Ç–∞–≤–ª–µ–Ω–æ –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–ø–¥–µ–π—Ç–∞

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
if not OPENROUTER_API_KEY:
    raise RuntimeError("‚ùå –ù–µ –∑–∞–¥–∞–Ω OPENROUTER_API_KEY")

MODELS = [
    "deepseek/deepseek-r1-0528:free",
    "deepseek/deepseek-chat-v3.1:free",
    "deepseek/deepseek-r1-0528-qwen3-8b:free",
    "tngtech/deepseek-r1t2-chimera:free",
    "mistralai/mistral-small-3.2-24b-instruct:free",
    "deepseek/deepseek-r1:free",
    "qwen/qwen3-14b:free",
    "mistralai/mistral-nemo:free"
]

logging.basicConfig(level=logging.INFO)

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫—ç—à–∞ –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π ---
CACHE_FILE = "data/response_cache.json"
os.makedirs("data", exist_ok=True)

excluded_models = {}  # –º–æ–¥–µ–ª—å: datetime, –¥–æ –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
last_working_model = None
MODEL_COOLDOWN_HOURS = 2

def load_cache(user_id, text):
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get(user_id, {}).get(text)
    return None

def save_cache(user_id, text, answer):
    data = {}
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    if user_id not in data:
        data[user_id] = {}
    data[user_id][text] = answer
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# --- –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ OpenRouter ---
async def ask_openrouter_single(session, user_id, messages, model):
    url = "https://openrouter.ai/api/v1/chat/completions"
    payload = {"model": model, "messages": messages, "temperature": 0.7}
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "HTTP-Referer": "https://iskin-ra.fly.dev",
        "X-Title": "iskin-ra",
    }

    async with session.post(url, headers=headers, json=payload) as resp:
        text_resp = await resp.text()
        if resp.status != 200:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ API {resp.status}: {text_resp}")
            raise Exception(f"{resp.status}: {text_resp}")

        data = await resp.json()
        if "choices" not in data or not data["choices"]:
            raise Exception(f"–ú–æ–¥–µ–ª—å {model} –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ choices")

        return data["choices"][0]["message"]["content"].strip()

# --- –û–±—ë—Ä—Ç–∫–∞ —Å –ø–µ—Ä–µ–±–æ—Ä–æ–º –º–æ–¥–µ–ª–µ–π –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º ---
async def safe_ask_openrouter(user_id, messages_payload):
    global excluded_models, last_working_model
    text_message = messages_payload[-1]["content"]
    cached = load_cache(user_id, text_message)
    if cached:
        logging.info("üíæ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç")
        return cached

    async with aiohttp.ClientSession() as session:
        now = datetime.now()
        usable_models = [m for m in MODELS if m not in excluded_models or excluded_models[m] <= now]

        # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ä–∞–±–æ—á—É—é –º–æ–¥–µ–ª—å
        if last_working_model and last_working_model in usable_models:
            usable_models.remove(last_working_model)
            usable_models = [last_working_model] + usable_models

        if not usable_models:
            logging.error("‚ö†Ô∏è –í—Å–µ –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
            return "‚ö†Ô∏è –í—Å–µ –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –ø–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ üôè"

        for model in usable_models:
            try:
                logging.info(f"üí° –ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª—å {model}")
                answer = await ask_openrouter_single(session, user_id, messages_payload, model)
                last_working_model = model
                save_cache(user_id, text_message, answer)
                break
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model} –≤—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–∫–ª—é—á–µ–Ω–∞: {e}")
                excluded_models[model] = datetime.now() + timedelta(hours=MODEL_COOLDOWN_HOURS)
        else:
            return "‚ö†Ô∏è –í—Å–µ –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –ø–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ üôè"

        # —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –∫—ç—à–∞ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        async def background_cache(model):
            try:
                ans = await ask_openrouter_single(session, user_id, messages_payload, model)
                save_cache(user_id, text_message, ans)
            except Exception as e:
                excluded_models[model] = datetime.now() + timedelta(hours=MODEL_COOLDOWN_HOURS)
                logging.warning(f"‚ö†Ô∏è –§–æ–Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å {model} –∏—Å–∫–ª—é—á–µ–Ω–∞: {e}")

        for model in usable_models:
            if model != last_working_model:
                asyncio.create_task(background_cache(model))

        return answer
