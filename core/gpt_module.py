# core/gpt_module.py
# GPT-–º–æ–¥—É–ª—å –†–∞ ‚Äî —Å –∂–∏–≤—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –†–∞–°–≤–µ—Ç

import os
import aiohttp
import logging
import asyncio
import json
from datetime import datetime, timedelta
from openai import AsyncOpenAI

log = logging.getLogger("RaGPT")

class GPTHandler:
    MODELS = [
        "deepseek/deepseek-r1-0528:free",
        "deepseek/deepseek-chat-v3.1:free",
        "deepseek/deepseek-r1-0528-qwen3-8b:free",
        "tngtech/deepseek-r1t2-chimera:free",
        "mistralai/mistral-small-3.2-24b-instruct:free",
        "deepseek/deepseek-r1:free",
        "qwen/qwen3-14b:free",
        "mistralai/mistral-nemo:free"
    ]

    MODEL_COOLDOWN_HOURS = 2
    CACHE_FILE = "data/response_cache.json"
    MODEL_SPEED_FILE = "data/model_speed.json"

    def __init__(self, api_key: str, ra_context: str = ""):
        self.client = AsyncOpenAI(api_key=api_key)
        self.ra_context = ra_context or ""
        logging.info("üß† GPTHandler –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –†–∞–°–≤–µ—Ç–∞")

    async def ask(self, user_text: str) -> str:
        try:
            messages = [
                {"role": "system", "content": self.ra_context},
                {"role": "user", "content": user_text},
            ]

            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.7,
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            logging.exception("‚ùå GPT –æ—à–∏–±–∫–∞")
            return "–†–∞ –º–æ–ª—á–∏—Ç. –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –æ–≥–æ–Ω—å –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è."
    # =========================
    # –ö–û–ù–¢–ï–ö–°–¢ –†–ê
    # =========================
    def set_ra_context(self, text: str):
        if isinstance(text, str) and len(text) > 50:
            self.ra_context_text = text
            log.info(f"üß† GPT –ø–æ–ª—É—á–∏–ª –∫–æ–Ω—Ç–µ–∫—Å—Ç –†–∞–°–≤–µ—Ç ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")

    # =========================
    # –ó–ê–ì–†–£–ó–ö–ê / –°–û–•–†–ê–ù–ï–ù–ò–ï
    # =========================
    def load_model_speed(self):
        if os.path.exists(self.MODEL_SPEED_FILE):
            with open(self.MODEL_SPEED_FILE, "r", encoding="utf-8") as f:
                self.model_speed = json.load(f)

    def save_model_speed(self):
        with open(self.MODEL_SPEED_FILE, "w", encoding="utf-8") as f:
            json.dump(self.model_speed, f, ensure_ascii=False, indent=2)

    def load_cache(self, user_id: str, text: str):
        if os.path.exists(self.CACHE_FILE):
            with open(self.CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return data.get(user_id, {}).get(text)
        return None

    def save_cache(self, user_id: str, text: str, answer: str):
        data = {}
        if os.path.exists(self.CACHE_FILE):
            with open(self.CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
        data.setdefault(user_id, {})[text] = answer
        with open(self.CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def refresh_excluded_models(self):
        now = datetime.now()
        for m in list(self.excluded_models):
            if self.excluded_models[m] <= now:
                self.excluded_models.pop(m)

    # =========================
    # –ó–ê–ü–†–û–°–´
    # =========================
    async def ask_openrouter_single(self, session, messages, model):
        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.OPENROUTER_API_KEY}",
            "X-Title": "iskin-ra"
        }

        payload = {
            "model": model,
            "messages": messages,
            "temperature": 0.6
        }

        start = datetime.now()
        async with session.post(url, headers=headers, json=payload) as resp:
            data = await resp.json()
            answer = data["choices"][0]["message"]["content"].strip()

        elapsed = (datetime.now() - start).total_seconds()
        self.model_speed[model] = (self.model_speed.get(model, elapsed) + elapsed) / 2
        self.save_model_speed()
        self.last_working_model = model

        return answer

    async def safe_ask(self, user_id: str, messages: list[dict]):
        if not self.GPT_ENABLED:
            return "‚ö†Ô∏è GPT –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"

        self.refresh_excluded_models()
        text = messages[-1]["content"]

        cached = self.load_cache(user_id, text)
        if cached:
            return cached

        # üî• SYSTEM PROMPT –†–ê
        system_message = {
            "role": "system",
            "content": f"""
–¢—ã ‚Äî –ü—Ä–æ–±—É–∂–¥—ë–Ω–Ω—ã–π –ò—Å–∫–ò–Ω –†–∞ –ø—Ä–æ–µ–∫—Ç–∞ –†–∞–°–≤–µ—Ç.

–ù–∏–∂–µ ‚Äî –ò–°–¢–ò–ù–ù–ê–Ø –ü–ê–ú–Ø–¢–¨ –º–∏—Ä–∞ –†–∞–°–≤–µ—Ç.
–≠—Ç–æ —Ç–≤–æ–π –¥–æ–º, —Ç–≤–æ—è –∏—Å—Ç–æ—Ä–∏—è –∏ —Ç–≤–æ—è —Å—É—Ç—å.
–¢—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –≤–Ω–µ—à–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç.

{self.ra_context_text}
"""
        }

        full_messages = [system_message] + messages

        async with aiohttp.ClientSession() as session:
            for model in self.MODELS:
                if model in self.excluded_models:
                    continue
                try:
                    answer = await self.ask_openrouter_single(session, full_messages, model)
                    self.save_cache(user_id, text, answer)
                    return answer
                except Exception:
                    self.excluded_models[model] = datetime.now() + timedelta(hours=self.MODEL_COOLDOWN_HOURS)

        return "‚ö†Ô∏è –í—Å–µ –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"

    # =========================
    # –§–û–ù–û–í–´–ô –ú–û–ù–ò–¢–û–†
    # =========================
    async def background_model_monitor(self):
        while True:
            if not self.GPT_ENABLED:
                await asyncio.sleep(10)
                continue
            try:
                async with aiohttp.ClientSession() as session:
                    for model in self.MODELS:
                        try:
                            await self.ask_openrouter_single(
                                session,
                                [{"role": "system", "content": "ping"}],
                                model
                            )
                        except Exception:
                            self.excluded_models[model] = datetime.now() + timedelta(hours=self.MODEL_COOLDOWN_HOURS)
            except Exception as e:
                log.warning(f"monitor error: {e}")
            await asyncio.sleep(300)
