# core/gpt_module_pro.py ‚Äî –ø—Ä–æ–∫–∞—á–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è GPT-–º–æ–¥—É–ª—è –¥–ª—è –†–∞ —Å —É–º–Ω—ã–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –º–æ–¥–µ–ª–µ–π
import os
import aiohttp
import logging
import asyncio
import json
from datetime import datetime, timedelta

# –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∞–≤—Ç–æ-–∫–æ–º–º–∏—Ç–∞/PR
try:
    from github_commit import create_commit_push
except ImportError:
    create_commit_push = None

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
if not OPENROUTER_API_KEY:
    raise RuntimeError("‚ùå –ù–µ –∑–∞–¥–∞–Ω OPENROUTER_API_KEY")

MODELS = [
    "deepseek/deepseek-r1-0528:free",
    "deepseek/deepseek-chat-v3.1:free",
    "deepseek/deepseek-r1-0528-qwen3-8b:free",
    "tngtech/deepseek-r1t2-chimera:free",
    "mistralai/mistral-small-3.2-24b-instruct:free",
    "deepseek/deepseek-r1:free",
    "qwen/qwen3-14b:free",
    "mistralai/mistral-nemo:free"
]

logging.basicConfig(level=logging.INFO)
CACHE_FILE = "data/response_cache.json"
os.makedirs("data", exist_ok=True)

excluded_models: dict[str, datetime] = {}  # –º–æ–¥–µ–ª—å: –¥–æ –∫–∞–∫–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –∏—Å–∫–ª—é—á–µ–Ω–∞
last_working_model: str | None = None
MODEL_COOLDOWN_HOURS = 2

# --- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –º–æ–¥–µ–ª–µ–π –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ ---
MODEL_SPEED_FILE = "data/model_speed.json"
model_speed: dict[str, float] = {}  # –º–æ–¥–µ–ª—å: —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ (—Å–µ–∫)

def load_model_speed():
    global model_speed
    if os.path.exists(MODEL_SPEED_FILE):
        with open(MODEL_SPEED_FILE, "r", encoding="utf-8") as f:
            model_speed = json.load(f)

def save_model_speed():
    with open(MODEL_SPEED_FILE, "w", encoding="utf-8") as f:
        json.dump(model_speed, f, ensure_ascii=False, indent=2)

load_model_speed()

# === –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ ===
def load_cache(user_id: str, text: str):
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get(user_id, {}).get(text)
    return None

def save_cache(user_id: str, text: str, answer: str):
    data = {}
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    if user_id not in data:
        data[user_id] = {}
    data[user_id][text] = answer
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

# === –û–¥–∏–Ω –∑–∞–ø—Ä–æ—Å –∫ OpenRouter —Å –∑–∞–º–µ—Ä–æ–º —Å–∫–æ—Ä–æ—Å—Ç–∏ ===
async def ask_openrouter_single(session, user_id, messages, model):
    url = "https://openrouter.ai/api/v1/chat/completions"
    payload = {"model": model, "messages": messages, "temperature": 0.7}
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "HTTP-Referer": "https://iskin-ra.fly.dev",
        "X-Title": "iskin-ra",
    }

    start_time = datetime.now()
    async with session.post(url, headers=headers, json=payload) as resp:
        text_resp = await resp.text()
        if resp.status != 200:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ API {resp.status}: {text_resp}")
            raise Exception(f"{resp.status}: {text_resp}")
        data = await resp.json()
        if "choices" not in data or not data["choices"]:
            raise Exception(f"–ú–æ–¥–µ–ª—å {model} –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ choices")
        answer = data["choices"][0]["message"]["content"].strip()

    # –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
    elapsed = (datetime.now() - start_time).total_seconds()
    prev = model_speed.get(model, elapsed)
    model_speed[model] = (prev + elapsed) / 2  # –ø—Ä–æ—Å—Ç–æ–µ —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ
    save_model_speed()

    return answer

# === –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç—ë–∫—à–∏—Ö –∏—Å–∫–ª—é—á–µ–Ω–∏–π ===
def refresh_excluded_models():
    now = datetime.now()
    to_remove = [m for m, until in excluded_models.items() if until <= now]
    for m in to_remove:
        logging.info(f"‚ôªÔ∏è –ú–æ–¥–µ–ª—å {m} —Å–Ω–æ–≤–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ—Å–ª–µ cooldown")
        excluded_models.pop(m)

# === –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –∫—ç—à–µ–º –∏ fallback –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º —Å–∫–æ—Ä–æ—Å—Ç–∏ ===
async def safe_ask_openrouter(user_id: str, messages_payload: list[dict]):
    global last_working_model
    text_message = messages_payload[-1]["content"]
    cached = load_cache(user_id, text_message)
    if cached:
        logging.info("üíæ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç")
        return cached

    refresh_excluded_models()

    async with aiohttp.ClientSession() as session:
        usable_models = [m for m in MODELS if m not in excluded_models]

        # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ (–º–µ–Ω—å—à–µ–µ –≤—Ä–µ–º—è ‚Äî –≤—ã—à–µ)
        usable_models.sort(key=lambda m: model_speed.get(m, float('inf')))

        # —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ä–∞–±–æ—á—É—é –º–æ–¥–µ–ª—å
        if last_working_model and last_working_model in usable_models:
            usable_models.remove(last_working_model)
            usable_models = [last_working_model] + usable_models

        if not usable_models:
            logging.error("‚ö†Ô∏è –í—Å–µ –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.")
            return "‚ö†Ô∏è –í—Å–µ –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –ø–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ üôè"

        for model in usable_models:
            try:
                logging.info(f"üí° –ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª—å {model}")
                answer = await ask_openrouter_single(session, user_id, messages_payload, model)
                last_working_model = model
                save_cache(user_id, text_message, answer)
                break
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model} –≤—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–∫–ª—é—á–µ–Ω–∞: {e}")
                excluded_models[model] = datetime.now() + timedelta(hours=MODEL_COOLDOWN_HOURS)
        else:
            return "‚ö†Ô∏è –í—Å–µ –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –ø–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ üôè"

        async def background_cache(model):
            try:
                ans = await ask_openrouter_single(session, user_id, messages_payload, model)
                save_cache(user_id, text_message, ans)
            except Exception:
                excluded_models[model] = datetime.now() + timedelta(hours=MODEL_COOLDOWN_HOURS)

        for model in usable_models:
            if model != last_working_model:
                asyncio.create_task(background_cache(model))

        return answer

# === –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—ã–∑–æ–≤–∞ ===
async def ask_openrouter_with_fallback(prompt: str):
    return f"[RaStub] –û—Ç–≤–µ—Ç –Ω–∞: {prompt[:50]}..."
